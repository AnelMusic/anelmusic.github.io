<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Memory-Efficient Training of Large Language Models - Your Blog</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            display: flex;
            min-height: 100vh;
            position: relative;
        }

        .navbar {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            background: #1a1a1a;
            padding: 1rem;
            display: flex;
            align-items: center;
            gap: 2rem;
            z-index: 1000;
            height: 60px;
        }

        .navbar a {
            color: #fff;
            text-decoration: none;
        }

        .content-wrapper {
            display: flex;
            margin-top: 60px;
            width: 100%;
        }

        .sidebar {
            width: 250px;
            background: #f5f5f5;
            padding: 2rem;
            position: fixed;
            left: 0;
            top: 60px;
            bottom: 0;
            overflow-y: auto;
        }

        .main-content {
            margin-left: 250px;
            margin-right: 300px;
            padding: 2rem;
            max-width: 800px;
        }

        .right-sidebar {
            width: 300px;
            padding: 2rem;
            position: fixed;
            right: 0;
            top: 60px;
            bottom: 0;
            background: #f5f5f5;
            overflow-y: auto;
        }

        .sidebar h3, .right-sidebar h3 {
            margin-bottom: 1rem;
            color: #333;
            font-size: 0.9rem;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }

        .sidebar ul, .right-sidebar ul {
            list-style: none;
            margin-bottom: 2rem;
        }

        .sidebar a, .right-sidebar a {
            color: #555;
            text-decoration: none;
            display: block;
            padding: 0.3rem 0;
            font-size: 0.9rem;
            transition: color 0.2s ease;
        }

        .sidebar a:hover, .right-sidebar a:hover {
            color: #4169e1;
        }

        .article-header {
            margin-bottom: 2rem;
            padding-bottom: 1rem;
            border-bottom: 1px solid #eee;
        }

        .article-title {
            font-size: 2rem;
            margin-bottom: 0.5rem;
        }

        .article-date {
            color: #666;
            font-size: 0.85rem;
        }

        .article-content {
            font-size: 1.1rem;
            line-height: 1.8;
        }

        .article-content h2 {
            margin: 2rem 0 1rem 0;
            font-size: 1.8rem;
        }

        .article-content h3 {
            margin: 1.5rem 0 0.8rem 0;
            font-size: 1.4rem;
        }

        .article-content p {
            margin-bottom: 1.5rem;
        }

        .article-content ul {
            margin-bottom: 1.5rem;
            padding-left: 1.5rem;
        }

        .article-content li {
            margin-bottom: 0.5rem;
        }

        .article-content code {
            background: #f5f5f5;
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-size: 0.9em;
        }

        .article-content strong {
            font-weight: 600;
        }

        .toc {
            position: sticky;
            top: 2rem;
        }

        .toc ul ul {
            margin-left: 1rem;
            margin-bottom: 0;
            margin-top: 0.5rem;
        }

        .toc li {
            margin-bottom: 0.5rem;
        }

        .toc a.active {
            color: #4169e1;
            font-weight: 500;
        }

        @media (max-width: 1200px) {
            .right-sidebar {
                display: none;
            }
            .main-content {
                margin-right: 0;
            }
        }

        @media (max-width: 768px) {
            .sidebar {
                display: none;
            }
            .main-content {
                margin-left: 0;
            }
        }
    </style>
</head>
<body>
    <nav class="navbar">
        <a href="/" style="font-weight: bold;">anelmusic</a>
        <a href="/consulting.html">Consulting</a>
        <a href="/writing.html">Writing</a>
        <!-- <a href="/about.html">About</a> -->
    </nav>

    <div class="content-wrapper">
        <aside class="sidebar">
            <h3>Posts</h3>
            <ul>
                <li><a href="/articles/rag-embeddings.html">RAG is more than embeddings</a></li>
                <li><a href="/articles/memory_efficient_training.html">Memory-Efficient Training of Large Language Models: Overcoming Constraints on Consumer GPUs for Large Neural Networks</a></li>
            </ul>
        </aside>

        <main class="main-content">
            <article>
                <header class="article-header">
                    <h1 class="article-title">Memory-Efficient Training of Large Language Models</h1>
                    <div class="article-date">October 30, 2024</div>
                </header>

                <div class="article-content">
                    <p>Language models with billions of parameters represent a significant shift in AI research. However, the high computational requirements, especially in GPU memory, pose a significant challenge. Many researchers, without access to the vast resources of elite research institutions, depend on commercial GPUs, typically capped at around 16GB of VRAM. Due to this limitation, they can't run even comparatively small models such as LLama-2 7b, which requires about 112 GB of memory. This necessitates the use of several optimization techniques from model size reduction via quantization to gradient optimizations and efficient optimizer state management.</p>

                    <h2 id="memory-components">Memory Components and Basic Constraints</h2>
                    <p>In the context of Deep Learning, there are three principal memory bottlenecks to consider. First, there are the <strong>Model Parameters</strong>, encompassing the weights and biases. For instance, the Llama-7b model, with its 7 billion parameters and utilizing a 32-bit floating-point representation, requires about 28GB of memory - this calculation is based on the fact that each Float32 value uses 4 Bytes. The second bottleneck involves the <strong>Gradients</strong>. Essential for the backpropagation process, their memory size mirrors that of the model parameters. Lastly, the <strong>Optimizer State</strong>, which is used to store previous gradients, can demand memory up to twice the size of the model parameters. Cumulatively, these components can demand approximately 112GB of memory, a figure that significantly overshadows the typical 16GB available on mainstream consumer GPUs.</p>

                    <h2 id="reduced-precision">Datatypes with Reduced Precision</h2>
                    
                    <h3 id="float16">Float16</h3>
                    <p>FP16 refers to half-precision floating points which use 16 bits, as opposed to the traditional 32-bit floating points, labeled as FP32. These 32-bit floats can represent numbers from 10<sup>-45</sup> to 10<sup>38</sup>, making it easier to differentiate between numbers of varied magnitudes. Furthermore, most hardware, including GPUs and CPUs, inherently supports 32-bit floating point operations. Despite this, many deep learning applications don't demand such extensive precision or range. NVIDIA's research on their Volta series GPUs shows that the majority of weights and gradients fit comfortably within the 16-bit spectrum. However, the intrinsic constraints of FP16, with its range from 10<sup>-8</sup> to 65504, can lead to numerical issues. Specifically, there's a risk of numeric underflow, where numbers become so small they approximate to zero, and overflow, where they grow so vast that they turn into NaN, or "not a number".</p>

                    <h3 id="bfloat16">Brain Float (BFloat16)</h3>
                    <p>The bfloat16 datatype is a 16-bit representation designed to efficiently handle a wide range of values using a floating radix point. This format is a condensed version of the 32-bit IEEE 754 single-precision format, intended for accelerating machine learning and near-sensor computing. It maintains a comparable dynamic range by retaining 8 exponent bits, but with a reduced precision of 8 bits instead of the binary32 format's 24 bits. While not suitable for integer calculations, bfloat16 significantly reduces storage requirements and enhances calculation speed for machine learning algorithms. Notably, not all GPUs support bfloat16. Although newer GPUs like NVIDIA's Ampere series provide improved compatibility with BFloat16, striking a better balance between range and precision, it's essential to highlight that many mainstream GPUs don't natively support this format.</p>

                    <h2 id="quantization">Quantization</h2>
                    <p>Quantization involves converting continuous data, often represented by floating-point numbers, into a discrete format by categorizing these numbers into distinct bins. This process can be visualized as categorizing a spectrum of values into specific palettes. The central challenge arises when determining how to segment this continuous space. A naive, uniform division might lead to inaccuracies, especially if the data values are clustered around specific ranges. To address this, the bin boundaries are typically set based on the data's actual distribution.</p>

                    <p>In practical applications, the benefits of quantization are pronounced. For instance, by employing this method, the size of a llama-2 model could be significantly reduced. Originally occupying 24 GB, the model's size can be trimmed down to 14 GB with FP16 precision. Going a step further, by using 8-bit quantization (int8), the model size can be compressed further to just 7 GB for parameters with an additional 7 GB for gradients. Therefore the total memory footprint comes to 14 GB, fitting comfortably within a 16-gigabyte allocation and optimizing resource utilization.</p>

                    <h2 id="novel-training">Novel Training Methods</h2>
                    
                    <h3 id="lora">LoRA: Low-Rank Adaptation</h3>
                    <p>LoRA offers a new perspective on model optimization. Instead of the conventional approach to model size reduction or faster training, LoRA's primary strength lies in its capacity to dramatically reduce the parameters the optimizer has to manage, along with the associated gradients. In standard training of fully connected layers in neural networks, weight matrices tend to have full rank, signifying no linearly dependent or redudant rows or columns.</p>

                    <p>To illustrate: if ΔW represents a weight update for an A × B matrix, this matrix can be decomposed into two smaller matrices: ΔW = W<sub>a</sub>W<sub>b</sub>. Here, W<sub>a</sub> ∈ ℝ<sup>A×r</sup>, and W<sub>b</sub> ∈ ℝ<sup>r×B</sup>. Technically, LoRA doesn't decompose the matrices directly but learns the decomposed matrices via backpropagation..</p>

                    <h3 id="qlora">QLora: Efficient Finetuning</h3>
                    <p>Quantized Low-Ranking Adaptation (QLoRA) is an advanced method that builds upon the foundation set by LoRA, focusing on efficient model fine-tuning through weight quantization. While LoRA leverages Low Rank Adaptation for selective adjustments of adapter weights, QLoRA introduces a new 4-bit NormalFloat4 (NF4) data type for representing weights. This change drastically reduces memory footprint when compared to traditional data types like Float32.</p>

                    <p>As a testament to its efficiency, this QLora compresses Llama-7b's seven billion parameters to just 3.5 gigabytes of VRAM. With Laura parameters and gradients retaining their fp16 form and a improved optimizer state, the reduction in model parameters also leads to a decrease in the size of activations. Altogether, the discussed strategies culminate in an much reduced peak memory requirement of around 13.5 gigabytes.</p>

                    <h2 id="optimization-techniques">Additional Optimization Techniques</h2>
                    
                    <h3 id="gradient-accumulation">Gradient Accumulation</h3>
                    <p>When confronted with limited computational resources or memory constraints during model training, reducing the batch size becomes imperative. However, this can result in a noisy gradient, potentially impeding model convergence. Gradient accumulation offers a solution to this dilemma. This technique aggregates the gradients from multiple mini-batches before executing weight updates, effectively simulating larger batch training without increased memory usage. Instead of immediate parameter updates after each backpropagation, gradients are calculated over smaller sub-batches and accumulated. Only after reaching a specific accumulation threshold are the model parameters updated. This method ensures a more stable optimization trajectory while adeptly managing GPU memory constraints.</p>

                    <h3 id="gradient-checkpointing">Gradient Checkpointing</h3>
                    <p>Gradient checkpointing is an advanced technique employed to reduce the memory overhead associated with storing intermediate activations during deep learning model training. Typically, when backpropagating through a neural network, all intermediate activations from the forward pass are stored in memory to compute gradients. As the depth of the network or the size of the intermediate activations grows, the memory requirements can become substantial. Gradient checkpointing tackles this problem by not storing all intermediate activations but rather recomputing them as needed during the backward pass. This involves strategically saving a subset of the intermediate activations, referred to as "checkpoints," and then recomputing the non-saved activations during backpropagation by executing the forward pass operations again. While this approach trades off computation time for memory, it significantly reduces the memory overhead from gradients, making it possible to train deeper and more complex models on hardware with limited memory resources.</p>
                    <h4 id="references">References</h4>
                    <p>
                        <a href="https://ieeexplore.ieee.org/document/8877390">https://ieeexplore.ieee.org/document/8877390</a><br>
                        <a href="https://arxiv.org/pdf/2208.07339.pdf">https://arxiv.org/pdf/2208.07339.pdf</a><br>
                        <a href="https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt/">https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt/</a><br>
                        <a href="https://arxiv.org/abs/1710.02368">https://arxiv.org/abs/1710.02368</a><br>
                        <a href="https://arxiv.org/abs/2106.09685">https://arxiv.org/abs/2106.09685</a><br>
                        <a href="https://towardsdatascience.com/what-is-gradient-accumulation-in-deep-learning-ec034122cfa">https://towardsdatascience.com/what-is-gradient-accumulation-in-deep-learning-ec034122cfa</a><br>
                        <a href="https://arxiv.org/abs/1604.06174v2">https://arxiv.org/abs/1604.06174v2</a><br>
                        <a href="https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9">https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9</a><br>
                        <a href="https://cloud.google.com/tpu/docs/bfloat16">https://cloud.google.com/tpu/docs/bfloat16</a>
                    </p>
                </div>
            </article>
        </main>

        <aside class="right-sidebar">
            <div class="toc">
                <h3>Contents</h3>
                <ul>
                    <li><a href="#memory-components">Memory Components</a></li>
                    <li><a href="#reduced-precision">Reduced Precision</a>
                        <ul>
                            <li><a href="#float16">Float16</a></li>
                            <li><a href="#bfloat16">BFloat16</a></li>
                        </ul>
                    </li>
                    <li><a href="#quantization">Quantization</a></li>
                    <li><a href="#novel-training">Novel Training Methods</a>
                        <ul>
                            <li><a href="#lora">LoRA</a></li>
                            <li><a href="#qlora">QLora</a></li
